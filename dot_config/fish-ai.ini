[fish-ai]
configuration = local-llama

[local-llama]
provider = self-hosted
model = llama3
server = http://localhost:11434/v1
